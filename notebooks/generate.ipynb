{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import note_seq\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from source.tokenizer import Tokenizer\n",
    "from source.transformer import Transformer\n",
    "from source.noteseqhelpers import token_sequence_to_note_sequence\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the .pt files in the models directory. Search recursively.\n",
    "model_paths = glob.glob(\"../models/**/*.pt\", recursive=True)\n",
    "model_paths.sort()\n",
    "print(f\"Found {len(model_paths)} models.\")\n",
    "\n",
    "# Widget to select a model.\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_paths,\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model path from the widget and load the model.\n",
    "model_path = model_dropdown.value\n",
    "assert os.path.exists(model_path), \"Model path does not exist.\"\n",
    "print(f\"Loading model from {model_path}\")\n",
    "model = Transformer.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"TristanBehrens/js-fakes-4bars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer.\n",
    "tokenizer_path = os.path.join(os.path.dirname(model_path), \"tokenizer.json\")\n",
    "assert os.path.exists(tokenizer_path), f\"Tokenizer path does not exist. {tokenizer_path}\"\n",
    "tokenizer = Tokenizer.from_config_file(tokenizer_path)\n",
    "print(tokenizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go from latent space to sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from a normal distribution.\n",
    "bottleneck_shape = model.get_bottleneck_shape()\n",
    "bottleneck_z = torch.randn(1, *bottleneck_shape)\n",
    "print(f\"bottleneck shape: {bottleneck_shape}, numbers {np.prod(bottleneck_shape)}\")\n",
    "\n",
    "for _ in range(4):\n",
    "    # Create the start sequence.\n",
    "    start_sequence = \"PIECE_START\"\n",
    "    start_sequence_indices = tokenizer.encode_sequence(start_sequence)\n",
    "    print(f\"Start sequence: {start_sequence}\")\n",
    "    print(f\"Start sequence indices: {start_sequence_indices}\")\n",
    "\n",
    "    result_ids = model.generate(\n",
    "        decoder_ids=start_sequence_indices,\n",
    "        max_new_tokens=512,\n",
    "        end_token_id=tokenizer.encode_token(\"TRACK_END\"),\n",
    "        bottleneck_condition=bottleneck_z, \n",
    "        temperature=0.5,\n",
    "        top_k=None\n",
    "    )[0]\n",
    "    #print(f\"Result ids: {result_ids}\")\n",
    "\n",
    "    result_sequence = tokenizer.decode_sequence(result_ids, join=True)\n",
    "    print(f\"Result sequence: {result_sequence}\")\n",
    "\n",
    "    note_sequence = token_sequence_to_note_sequence(result_sequence)\n",
    "    note_seq.plot_sequence(note_sequence)\n",
    "    note_seq.play_sequence(note_sequence, synth=note_seq.fluidsynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "split_dataset = load_dataset(dataset_id)\n",
    "\n",
    "def random_sample():\n",
    "    # Select a random sample.\n",
    "    random_index = random.randint(0, len(split_dataset[\"test\"]) - 1)\n",
    "    return split_dataset[\"test\"][random_index][\"text\"]\n",
    "\n",
    "random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = random_sample()\n",
    "encoder_ids = tokenizer.encode_sequence(sequence)\n",
    "print(f\"Sequence: {sequence}\")\n",
    "print(f\"Encoder ids: {encoder_ids}\")\n",
    "note_sequence = token_sequence_to_note_sequence(sequence)\n",
    "note_seq.plot_sequence(note_sequence)\n",
    "note_seq.play_sequence(note_sequence, synth=note_seq.fluidsynth)\n",
    "\n",
    "for _ in range(4):\n",
    "    # Create the start sequence.\n",
    "    start_sequence = \"PIECE_START\"\n",
    "    start_sequence_indices = tokenizer.encode_sequence(start_sequence)\n",
    "    print(f\"Start sequence: {start_sequence}\")\n",
    "    print(f\"Start sequence indices: {start_sequence_indices}\")\n",
    "\n",
    "    result_ids = model.generate(\n",
    "        decoder_ids=start_sequence_indices,\n",
    "        encoder_ids=encoder_ids,\n",
    "        max_new_tokens=512,\n",
    "        end_token_id=tokenizer.encode_token(\"TRACK_END\"),\n",
    "        temperature=0.2,\n",
    "        top_k=None\n",
    "    )[0]\n",
    "    #print(f\"Result ids: {result_ids}\")\n",
    "\n",
    "    result_sequence = tokenizer.decode_sequence(result_ids, join=True)\n",
    "    print(f\"Result sequence: {result_sequence}\")\n",
    "\n",
    "    note_sequence = token_sequence_to_note_sequence(result_sequence)\n",
    "    note_seq.plot_sequence(note_sequence)\n",
    "    note_seq.play_sequence(note_sequence, synth=note_seq.fluidsynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate between two latent vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from a normal distribution.\n",
    "bottleneck_shape = model.get_bottleneck_shape()\n",
    "bottleneck_z_1 = torch.randn(1, *bottleneck_shape)\n",
    "bottleneck_z_2 = torch.randn(1, *bottleneck_shape)\n",
    "print(f\"bottleneck shape: {bottleneck_z_1.shape}, numbers {np.prod(bottleneck_z_1.shape)}\")\n",
    "print(f\"bottleneck shape: {bottleneck_z_2.shape}, numbers {np.prod(bottleneck_z_2.shape)}\")\n",
    "\n",
    "def slerp(z1, z2, steps):\n",
    "    # Ensure that z1 and z2 are the same shape\n",
    "    assert z1.shape == z2.shape, \"z1 and z2 must have the same shape\"\n",
    "    \n",
    "    # Flatten the last two dimensions for easier computations\n",
    "    z1_flattened = z1.view(z1.shape[0], -1)\n",
    "    z2_flattened = z2.view(z2.shape[0], -1)\n",
    "\n",
    "    # Normalize the tensors along the last dimension\n",
    "    z1_norm = z1_flattened / z1_flattened.norm(dim=-1, keepdim=True)\n",
    "    z2_norm = z2_flattened / z2_flattened.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Dot product between the normalized tensors along the last dimension\n",
    "    cos_omega = torch.sum(z1_norm * z2_norm, dim=-1)\n",
    "    # Clamp values to handle numerical imprecisions\n",
    "    cos_omega = torch.clamp(cos_omega, -1.0, 1.0)\n",
    "    omega = torch.acos(cos_omega)\n",
    "    sin_omega = torch.sin(omega)\n",
    "\n",
    "    t_values = torch.linspace(0, 1, steps).to(z1.device)\n",
    "    result = []\n",
    "\n",
    "    for t in t_values:\n",
    "        interpolate = (torch.sin((1 - t) * omega).unsqueeze(-1) / sin_omega.unsqueeze(-1)) * z1_flattened + (torch.sin(t * omega).unsqueeze(-1) / sin_omega.unsqueeze(-1)) * z2_flattened\n",
    "        # Reshape to the original shape of z1 and z2\n",
    "        interpolate = interpolate.view(z1.shape)\n",
    "        result.append(interpolate)\n",
    "\n",
    "    # Concatenate along a new first dimension\n",
    "    return torch.stack(result)\n",
    "\n",
    "z_list = slerp(bottleneck_z_1, bottleneck_z_2, 4)\n",
    "print(f\"z_list shape: {z_list.shape}\")\n",
    "for z in z_list:\n",
    "    # Create the start sequence.\n",
    "    start_sequence = \"PIECE_START\"\n",
    "    start_sequence_indices = tokenizer.encode_sequence(start_sequence)\n",
    "    print(f\"Start sequence: {start_sequence}\")\n",
    "    print(f\"Start sequence indices: {start_sequence_indices}\")\n",
    "\n",
    "    result_ids = model.generate(\n",
    "        decoder_ids=start_sequence_indices,\n",
    "        max_new_tokens=512,\n",
    "        end_token_id=tokenizer.encode_token(\"TRACK_END\"),\n",
    "        bottleneck_condition=z, \n",
    "        temperature=1.0,\n",
    "        top_k=None\n",
    "    )[0]\n",
    "    #print(f\"Result ids: {result_ids}\")\n",
    "\n",
    "    result_sequence = tokenizer.decode_sequence(result_ids, join=True)\n",
    "    print(f\"Result sequence: {result_sequence}\")\n",
    "\n",
    "    note_sequence = token_sequence_to_note_sequence(result_sequence)\n",
    "    note_seq.plot_sequence(note_sequence)\n",
    "    note_seq.play_sequence(note_sequence, synth=note_seq.fluidsynth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
